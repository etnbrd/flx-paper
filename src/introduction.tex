\section{Introduction}

The growth of web platforms is partially due to Internet's capacity to allow very quick releases of a minimal viable product (MVP).
In a matter of hours, it is possible to release a prototype and start gathering a user community around.
\textit{``Release early, release often''}, and \textit{``Fail fast''} are the punchlines of the web entrepreneurial community.
It is crucial for the prosperity of such project to quickly validate that the proposed solution meets the needs of its users.
Indeed, the lack of market need is the number one reason for startup failure.\ftnt{https://www.cbinsights.com/blog/startup-failure-post-mortem/}
That is why the development team quickly concretises an MVP and iterates on it using a feature-driven, monolithic approach.
Such as proposed by imperative languages like Java or Ruby.

If the service successfully complies with users requirements, its community might grow with its popularity.
If the service can quickly respond to this growth, it is scalable.
However, it is difficult to develop scalable applications with the feature-driven approach mentioned above.
Eventually this growth requires to discard the initial monolithic approach to adopt a more efficient processing model instead.
Many of the most efficient models distribute the system on a cluster of commodity machines \cite{Fox1997}.
MapReduce \cite{Dean2008} and the Staged Event-driven Architecture (SEDA) \cite{Welsh2000} are famous examples of that trend. %, using a pipeline architecture.
Once split, the service parts are connected by an asynchronous messaging system.
Many tools have been developed to express and manage these service parts and their communications.
We can cite Spark \cite{Zaharia2010}, MillWheel \cite{Akidau2013}, Timestream \cite{Qian2013}, Naiad \cite{McSherry} and Storm \cite{Toshniwal2014}, and many others.
However, these tools impose specific interfaces and languages, different from the initial monolithic approach.
It requires the development team either to be trained or to hire experts, and to start over the initial code base.
This shift causes the development team to spend development resources in background without adding visible value for the users.
It is a risk for the economic evolution of the project as the number two and three reasons for startup failures are running out of cash, and missing the right competences.
% \ftnt{https://www.cbinsights.com/blog/startup-failure-post-mortem/}

To lift the risks described above, we propose a tool to compile the initial code base into a high-level language compatible with the more efficient processing model.
We focus on web applications driven by users requests, developed in Javascript using the \textit{Node.js} execution environment.

Javascript is increasingly used to develop web applications.
It is the most used language on Github\ftnt{http://githut.info/}, and the second one on StackOverflow\ftnt{http://stackoverflow.com/tags}.
We think that it is possible to analyze this type of application as a stream of requests, passing through a pipeline of stages.
Indeed, the event-loop used in \textit{Node.js} is very similar to a pipeline architecture.
We propose a compiler to transform a monolithic Javascript application into a network of autonomous parts communicating by message streams.
We named these parts \textit{fluxions}, by contraction between a flux and a function.
We are interested in the problems arising from the isolation of the global memory into these fluxions.
%This tool and its runtime aim not to modify the existing code, but rely on a high-level language expression over the initial code base.
We present an early version of this tool as a proof of concept for this compilation approach.
We start by describing in section \ref{section:model} the execution environment targeted by this compiler.
Then, we present the compiler in section \ref{section:compiler}, and its evaluation in section \ref{section:evaluation}.
We compare our work with related works in section \ref{section:related}.
And finally, we conclude this paper.